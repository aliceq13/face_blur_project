# -*- coding: utf-8 -*-
"""
ë¹„ë””ì˜¤ ë¸”ëŸ¬ ì²˜ë¦¬ ëª¨ë“ˆ (YOLO Face v11s + AdaFace ViT-12M + Efficient Single-Pass)

ì´ ëª¨ë“ˆì€ ì›ë³¸ ë¹„ë””ì˜¤ì™€ ì–¼êµ´ ì •ë³´ë¥¼ ë°›ì•„, ì§€ì •ëœ ì–¼êµ´ì„ ë¸”ëŸ¬ ì²˜ë¦¬í•œ ìƒˆë¡œìš´ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ìˆ :
1. YOLO Face v11s: ë¹ ë¥¸ ì‹¤ì‹œê°„ ì–¼êµ´ ê°ì§€
2. AdaFace ViT-12M: ê³ ì •ë°€ ì–¼êµ´ ì¸ì‹
3. Multi-Embedding Matching: ì—¬ëŸ¬ ì„ë² ë”©ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
4. Efficient Single-Pass: í•œ ë²ˆì˜ ìˆœíšŒë¡œ ì²˜ë¦¬ ì™„ë£Œ
5. ì ì‘í˜• ë¸”ëŸ¬: ì–¼êµ´ í¬ê¸°ì— ë”°ë¥¸ ë™ì  ë¸”ëŸ¬ ê°•ë„
"""

import cv2
import numpy as np
import torch
from typing import List, Dict, Optional
import logging
import os
import subprocess
import shutil
import gc

logger = logging.getLogger(__name__)


class VideoBlurrer:
    """
    ë¹„ë””ì˜¤ ë¸”ëŸ¬ ì²˜ë¦¬ í´ë˜ìŠ¤

    ë§¤ í”„ë ˆì„ë§ˆë‹¤:
    1. YOLOë¡œ ì–¼êµ´ ê°ì§€
    2. AdaFaceë¡œ ì„ë² ë”© ì¶”ì¶œ
    3. íƒ€ê²Ÿê³¼ ë¹„êµ (Multi-Embedding Matching)
    4. íƒ€ê²Ÿì´ ì•„ë‹ˆë©´ ë¸”ëŸ¬ ì ìš©
    5. í”„ë ˆì„ ì €ì¥
    """

    def __init__(
        self,
        yolo_model_path: str,
        device: str = 'auto',
        threshold: float = 0.92,  # ë§¤ìš° ì—„ê²©í•œ ë§¤ì¹­ (0.92)
        use_multi_embedding: bool = False  # Multi-Embedding ë¹„í™œì„±í™” (ë©”ì¸ë§Œ ì‚¬ìš©)
    ):
        self.threshold = threshold
        self.use_multi_embedding = use_multi_embedding

        if device == 'auto':
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        logger.info(f"ğŸš€ Initializing VideoBlurrer on {self.device}")

        # 1. YOLO Face v11s ë¡œë“œ
        from ultralytics import YOLO
        self.yolo_model = YOLO(yolo_model_path)
        self.yolo_model.to(self.device)
        logger.info(f"âœ… YOLO Face model loaded: {yolo_model_path}")

        # 2. AdaFace ViT-12M ë¡œë“œ
        try:
            from .adaface_wrapper import AdaFaceWrapper
            from django.conf import settings

            model_path = str(getattr(settings, 'ADAFACE_MODEL_PATH', None))
            model_arch = getattr(settings, 'ADAFACE_ARCHITECTURE', 'ir_101')

            if not model_path or not os.path.exists(model_path):
                raise FileNotFoundError(f"AdaFace model not found: {model_path}")

            self.face_recognizer = AdaFaceWrapper(
                model_path=model_path,
                architecture=model_arch,
                device=self.device
            )
            logger.info(f"âœ… AdaFace {model_arch} loaded from {model_path}")

        except Exception as e:
            logger.error(f"âŒ Failed to load AdaFace: {e}")
            self.face_recognizer = None

    def _get_embedding(self, face_img_bgr: np.ndarray) -> Optional[np.ndarray]:
        """ì–¼êµ´ ì´ë¯¸ì§€ì—ì„œ ì„ë² ë”© ì¶”ì¶œ"""
        if self.face_recognizer is None:
            return None
        return self.face_recognizer.get_embedding(face_img_bgr)

    def _is_target_face(
        self,
        face_img_bgr: np.ndarray,
        target_embeddings: List[np.ndarray],
        multi_embeddings: List[List[np.ndarray]],
        frame_idx: int = 0
    ) -> tuple:
        """
        ì–¼êµ´ì´ íƒ€ê²Ÿì¸ì§€ í™•ì¸ (Multi-Embedding Matching)

        Args:
            face_img_bgr: ì–¼êµ´ ì´ë¯¸ì§€ (BGR)
            target_embeddings: íƒ€ê²Ÿì˜ ë©”ì¸ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
            multi_embeddings: íƒ€ê²Ÿì˜ Multi-Thumbnail ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
            frame_idx: í˜„ì¬ í”„ë ˆì„ ë²ˆí˜¸ (ë¡œê¹…ìš©)

        Returns:
            (is_target, max_sim): (íƒ€ê²Ÿ ì—¬ë¶€, ìµœëŒ€ ìœ ì‚¬ë„)
        """
        if (not target_embeddings and not multi_embeddings) or face_img_bgr is None or face_img_bgr.size == 0:
            return False, 0.0

        # ì„ë² ë”© ì¶”ì¶œ
        embedding = self._get_embedding(face_img_bgr)
        if embedding is None:
            return False, 0.0

        # íƒ€ê²Ÿë“¤ê³¼ ë¹„êµ
        max_sim = -1.0
        max_sim_type = "none"

        # 1. ë©”ì¸ ì„ë² ë”©ê³¼ ë¹„êµ (í•­ìƒ ìˆ˜í–‰)
        for idx, target_emb in enumerate(target_embeddings):
            sim = np.dot(embedding, target_emb)
            if sim > max_sim:
                max_sim = sim
                max_sim_type = f"main_{idx}"

        # 2. Multi-Thumbnail ì„ë² ë”©ê³¼ ë¹„êµ (ì˜µì…˜)
        if self.use_multi_embedding:
            for target_idx, multi_emb_list in enumerate(multi_embeddings):
                for emb_idx, emb in enumerate(multi_emb_list):
                    emb_array = np.array(emb)
                    # L2 ì •ê·œí™”
                    emb_array = emb_array / (np.linalg.norm(emb_array) + 1e-8)
                    sim = np.dot(embedding, emb_array)
                    if sim > max_sim:
                        max_sim = sim
                        max_sim_type = f"multi_{target_idx}_{emb_idx}"

        # ì„ê³„ê°’ ë¹„êµ
        is_target = max_sim > self.threshold

        # ìƒì„¸ ë¡œê·¸ (100 í”„ë ˆì„ë§ˆë‹¤ ë˜ëŠ” threshold ê·¼ì²˜)
        if frame_idx % 100 == 0 or (max_sim > self.threshold - 0.1 and max_sim < self.threshold + 0.1):
            logger.info(
                f"  Frame {frame_idx}: similarity={max_sim:.3f} (threshold={self.threshold:.3f}, "
                f"type={max_sim_type}) â†’ {'PRESERVE' if is_target else 'BLUR'}"
            )

        return is_target, max_sim

    def _apply_blur(
        self,
        frame: np.ndarray,
        x1: int, y1: int, x2: int, y2: int,
        blur_type: str = 'pixelate',
        blur_strength: int = 15,
        padding: int = 20
    ) -> np.ndarray:
        """
        ì–¼êµ´ ì˜ì—­ì— ë¸”ëŸ¬ ì ìš©

        Args:
            frame: ì›ë³¸ í”„ë ˆì„
            x1, y1, x2, y2: ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤
            blur_type: 'pixelate' ë˜ëŠ” 'gaussian'
            blur_strength: ë¸”ëŸ¬ ê°•ë„
            padding: ì–¼êµ´ ì£¼ë³€ íŒ¨ë”©

        Returns:
            ë¸”ëŸ¬ ì²˜ë¦¬ëœ í”„ë ˆì„
        """
        h, w = frame.shape[:2]

        # íŒ¨ë”© ì¶”ê°€
        x1_pad = max(0, x1 - padding)
        y1_pad = max(0, y1 - padding)
        x2_pad = min(w, x2 + padding)
        y2_pad = min(h, y2 + padding)

        roi = frame[y1_pad:y2_pad, x1_pad:x2_pad]

        if roi.size == 0:
            return frame

        if blur_type == 'pixelate':
            # í”½ì…€í™” (ëª¨ìì´í¬)
            roi_h, roi_w = roi.shape[:2]
            factor = max(1, max(roi_h, roi_w) // blur_strength)

            if factor > 1:
                small = cv2.resize(roi, (roi_w // factor, roi_h // factor), interpolation=cv2.INTER_NEAREST)
                blurred_roi = cv2.resize(small, (roi_w, roi_h), interpolation=cv2.INTER_NEAREST)
            else:
                # ë„ˆë¬´ ì‘ìœ¼ë©´ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ fallback
                k_w = max((x2_pad - x1_pad) // 3 | 1, 3)
                k_h = max((y2_pad - y1_pad) // 3 | 1, 3)
                blurred_roi = cv2.GaussianBlur(roi, (k_w, k_h), 30)

        elif blur_type == 'gaussian':
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            k_w = max((x2_pad - x1_pad) // 3 | 1, 3)
            k_h = max((y2_pad - y1_pad) // 3 | 1, 3)
            blurred_roi = cv2.GaussianBlur(roi, (k_w, k_h), 30)

        else:
            # ê¸°ë³¸ê°’: ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            k_w = max((x2_pad - x1_pad) // 3 | 1, 3)
            k_h = max((y2_pad - y1_pad) // 3 | 1, 3)
            blurred_roi = cv2.GaussianBlur(roi, (k_w, k_h), 30)

        frame[y1_pad:y2_pad, x1_pad:x2_pad] = blurred_roi
        return frame

    def process_video(
        self,
        video_path: str,
        output_path: str,
        face_models: List[Dict],
        progress_callback: Optional[callable] = None,
        blur_type: str = 'pixelate',
        blur_strength: int = 15,
        threshold: float = 0.6
    ) -> bool:
        """
        ë¹„ë””ì˜¤ ë¸”ëŸ¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            output_path: ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            face_models: Face ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ [{'id', 'embedding', 'embeddings', 'is_blurred'}, ...]
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
            blur_type: 'pixelate' ë˜ëŠ” 'gaussian'
            blur_strength: ë¸”ëŸ¬ ê°•ë„ (ë†’ì„ìˆ˜ë¡ ì•½í•¨)
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.6)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info("=" * 80)
            logger.info("ğŸ¬ Starting Video Blur Processing (Single-Pass)")
            logger.info("=" * 80)

            # threshold ì—…ë°ì´íŠ¸
            self.threshold = threshold

            # 1. íƒ€ê²Ÿ ì„ë² ë”© ì¤€ë¹„ (is_blurred=Falseì¸ ì–¼êµ´ë“¤ - ë³´ì¡´í•  ì–¼êµ´ë“¤)
            target_embeddings = []
            multi_embeddings = []

            logger.info(f"ğŸ“‹ Total face_models: {len(face_models)}")

            for fm in face_models:
                is_blurred_val = fm.get('is_blurred', True)
                logger.info(f"Face {fm.get('id')}: is_blurred={is_blurred_val}")

                if not is_blurred_val:  # is_blurred=False (ë³´ì¡´í•  íƒ€ê²Ÿ)
                    # ë©”ì¸ ì„ë² ë”©
                    emb = np.array(fm['embedding'])
                    emb = emb / (np.linalg.norm(emb) + 1e-8)  # L2 ì •ê·œí™”
                    target_embeddings.append(emb)

                    # Multi-Thumbnail ì„ë² ë”©
                    if 'embeddings' in fm and fm['embeddings']:
                        multi_embeddings.append(fm['embeddings'])

                    logger.info(f"  â†’ Added as TARGET (will be PRESERVED - NO blur)")

            logger.info(f"ğŸ¯ Targets to preserve: {len(target_embeddings)}")
            if target_embeddings:
                logger.info("âœ… Selected faces will be preserved (not blurred)")
            else:
                logger.warning("âš ï¸  No target faces selected! ALL faces will be blurred!")

            # 2. ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS) or 30.0

            logger.info(f"ğŸ“¹ Video: {width}x{height}, {fps} fps, {total_frames} frames")

            # 3. VideoWriter ìƒì„±
            temp_output = output_path + ".temp.mp4"
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))

            if not out.isOpened():
                raise RuntimeError(f"Failed to open VideoWriter: {temp_output}")

            # 4. í”„ë ˆì„ë³„ ì²˜ë¦¬
            frame_idx = 0
            blur_count = 0
            skip_count = 0
            preserved_count = 0

            logger.info("ğŸï¸  Processing frames...")

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # YOLO ì–¼êµ´ ê°ì§€
                results = self.yolo_model(frame, conf=0.4, verbose=False, device=self.device)

                if results and results[0].boxes:
                    for box in results[0].boxes:
                        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())

                        # ì¢Œí‘œ ë³´ì •
                        x1, y1 = max(0, x1), max(0, y1)
                        x2, y2 = min(width, x2), min(height, y2)

                        # ë„ˆë¬´ ì‘ì€ ì–¼êµ´ì€ ìŠ¤í‚µ
                        if (x2 - x1) < 20 or (y2 - y1) < 20:
                            skip_count += 1
                            continue

                        # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
                        face_img_bgr = frame[y1:y2, x1:x2].copy()

                        # â­ ê¸°ë³¸ê°’: ë¸”ëŸ¬ ì ìš©
                        should_blur = True

                        # íƒ€ê²Ÿê³¼ ë¹„êµ (íƒ€ê²Ÿì´ ìˆëŠ” ê²½ìš°ë§Œ)
                        if target_embeddings or multi_embeddings:
                            is_target, similarity = self._is_target_face(
                                face_img_bgr,
                                target_embeddings,
                                multi_embeddings,
                                frame_idx
                            )

                            if is_target:
                                # íƒ€ê²Ÿ ì–¼êµ´ â†’ ë¸”ëŸ¬ í•´ì œ
                                should_blur = False
                                preserved_count += 1

                        # ë¸”ëŸ¬ ì ìš©
                        if should_blur:
                            frame = self._apply_blur(
                                frame, x1, y1, x2, y2,
                                blur_type=blur_type,
                                blur_strength=blur_strength
                            )
                            blur_count += 1

                # í”„ë ˆì„ ì €ì¥
                out.write(frame)

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                frame_idx += 1
                if progress_callback and frame_idx % 30 == 0:
                    pct = int((frame_idx / total_frames) * 90)
                    pct = min(pct, 90)
                    progress_callback(pct)

                    if frame_idx % 300 == 0:
                        logger.info(
                            f"ğŸ“Š Processed {frame_idx}/{total_frames} frames | "
                            f"Blurred: {blur_count} | Preserved: {preserved_count} | Skipped: {skip_count}"
                        )

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if frame_idx % 500 == 0:
                    gc.collect()

            cap.release()
            out.release()

            logger.info(f"âœ… Processing completed: {frame_idx} frames")
            logger.info(f"ğŸ“Š Blurred: {blur_count}, Preserved: {preserved_count}, Skipped: {skip_count}")

            # 5. H.264 ì¸ì½”ë”©
            logger.info("ğŸï¸  Encoding to H.264...")
            self._encode_h264(temp_output, output_path)

            if progress_callback:
                progress_callback(100)

            logger.info("=" * 80)
            logger.info("âœ… Video processing completed successfully!")
            logger.info("=" * 80)

            return True

        except Exception as e:
            logger.error(f"âŒ Video processing failed: {e}", exc_info=True)
            return False

    def _encode_h264(self, input_path: str, output_path: str):
        """FFmpegë¡œ H.264 ì¸ì½”ë”©"""
        try:
            cmd = [
                'ffmpeg', '-y',
                '-i', input_path,
                '-c:v', 'libx264',
                '-preset', 'fast',
                '-crf', '23',
                '-c:a', 'aac',
                '-movflags', '+faststart',
                output_path
            ]

            logger.info(f"ğŸ¬ Running FFmpeg: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(input_path):
                os.remove(input_path)

            logger.info("âœ… FFmpeg encoding completed")

        except subprocess.TimeoutExpired:
            logger.error("âŒ FFmpeg encoding timeout")
            # Fallback: ì„ì‹œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if os.path.exists(input_path) and not os.path.exists(output_path):
                shutil.move(input_path, output_path)
                logger.warning("âš ï¸  Using temp file as output (FFmpeg timeout)")

        except Exception as e:
            logger.error(f"âŒ FFmpeg encoding failed: {e}")
            # Fallback: ì„ì‹œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if os.path.exists(input_path) and not os.path.exists(output_path):
                shutil.move(input_path, output_path)
                logger.warning("âš ï¸  Using temp file as output (FFmpeg failed)")
