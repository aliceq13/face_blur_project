# -*- coding: utf-8 -*-
"""
ë¹„ë””ì˜¤ ë¸”ëŸ¬ ì²˜ë¦¬ ëª¨ë“ˆ (Optimized with Saved Tracking Data)

ì´ ëª¨ë“ˆì€ ì›ë³¸ ë¹„ë””ì˜¤ì™€ ì–¼êµ´ ì •ë³´ë¥¼ ë°›ì•„, ì§€ì •ëœ ì–¼êµ´ì„ ë¸”ëŸ¬ ì²˜ë¦¬í•œ ìƒˆë¡œìš´ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

í•µì‹¬ ìµœì í™”:
1. Saved Frame-level BBox: ë¶„ì„ ë‹¨ê³„ì—ì„œ ì €ì¥í•œ bbox ì‚¬ìš© (YOLO ì¬ì‹¤í–‰ ë¶ˆí•„ìš”)
2. Instance-based Blur: instance_id ê¸°ë°˜ ë¸”ëŸ¬ ê²°ì • (AdaFace ì¬ì¶”ì¶œ ë¶ˆí•„ìš”)
3. Efficient Single-Pass: í•œ ë²ˆì˜ ìˆœíšŒë¡œ ì²˜ë¦¬ ì™„ë£Œ
4. ì ì‘í˜• ë¸”ëŸ¬: ì–¼êµ´ í¬ê¸°ì— ë”°ë¥¸ ë™ì  ë¸”ëŸ¬ ê°•ë„

ì´ì „ ë°©ì‹ ëŒ€ë¹„ ê°œì„ :
- âŒ ì´ì „: ë§¤ í”„ë ˆì„ YOLO ì–¼êµ´ ê°ì§€ â†’ AdaFace ì„ë² ë”© ì¶”ì¶œ â†’ ìœ ì‚¬ë„ ë¹„êµ
- âœ… í˜„ì¬: ì €ì¥ëœ bbox ì¡°íšŒ â†’ instance_id ê¸°ë°˜ ë¸”ëŸ¬ ì ìš©
"""

import cv2
import numpy as np
import torch
from typing import List, Dict, Optional
import logging
import os
import subprocess
import shutil
import gc

logger = logging.getLogger(__name__)


class VideoBlurrer:
    """
    ë¹„ë””ì˜¤ ë¸”ëŸ¬ ì²˜ë¦¬ í´ë˜ìŠ¤ (Optimized)

    ë§¤ í”„ë ˆì„ë§ˆë‹¤:
    1. ì €ì¥ëœ bbox ë°ì´í„° ì¡°íšŒ (frame_dataì—ì„œ)
    2. instance_id ê¸°ë°˜ ë¸”ëŸ¬ ì—¬ë¶€ íŒë‹¨ (is_blurred í•„ë“œ)
    3. is_blurred=Trueì´ë©´ ë¸”ëŸ¬ ì ìš©, Falseì´ë©´ ë³´ì¡´
    4. í”„ë ˆì„ ì €ì¥

    ì£¼ìš” ê°œì„ ì :
    - YOLO ì¬ì‹¤í–‰ ë¶ˆí•„ìš” (ì´ë¯¸ ì €ì¥ëœ bbox ì‚¬ìš©)
    - AdaFace ì¬ì¶”ì¶œ ë¶ˆí•„ìš” (instance_idë¡œ ì§ì ‘ íŒë‹¨)
    - ì²˜ë¦¬ ì†ë„ ëŒ€í­ í–¥ìƒ
    """

    def __init__(
        self,
        yolo_model_path: str,
        device: str = 'auto',
        threshold: float = 0.92,  # ë§¤ìš° ì—„ê²©í•œ ë§¤ì¹­ (0.92)
        use_multi_embedding: bool = False  # Multi-Embedding ë¹„í™œì„±í™” (ë©”ì¸ë§Œ ì‚¬ìš©)
    ):
        self.threshold = threshold
        self.use_multi_embedding = use_multi_embedding

        if device == 'auto':
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        logger.info(f"ğŸš€ Initializing VideoBlurrer on {self.device}")

        # 1. YOLO Face v11s ë¡œë“œ
        from ultralytics import YOLO
        self.yolo_model = YOLO(yolo_model_path)
        self.yolo_model.to(self.device)
        logger.info(f"âœ… YOLO Face model loaded: {yolo_model_path}")

        # 2. AdaFace ViT-12M ë¡œë“œ
        try:
            from .adaface_wrapper import AdaFaceWrapper
            from django.conf import settings

            model_path = str(getattr(settings, 'ADAFACE_MODEL_PATH', None))
            model_arch = getattr(settings, 'ADAFACE_ARCHITECTURE', 'ir_101')

            if not model_path or not os.path.exists(model_path):
                raise FileNotFoundError(f"AdaFace model not found: {model_path}")

            self.face_recognizer = AdaFaceWrapper(
                model_path=model_path,
                architecture=model_arch,
                device=self.device
            )
            logger.info(f"âœ… AdaFace {model_arch} loaded from {model_path}")

        except Exception as e:
            logger.error(f"âŒ Failed to load AdaFace: {e}")
            self.face_recognizer = None

    def _get_embedding(self, face_img_bgr: np.ndarray) -> Optional[np.ndarray]:
        """ì–¼êµ´ ì´ë¯¸ì§€ì—ì„œ ì„ë² ë”© ì¶”ì¶œ"""
        if self.face_recognizer is None:
            return None
        return self.face_recognizer.get_embedding(face_img_bgr)

    def _is_target_face(
        self,
        face_img_bgr: np.ndarray,
        target_embeddings: List[np.ndarray],
        multi_embeddings: List[List[np.ndarray]],
        frame_idx: int = 0
    ) -> tuple:
        """
        ì–¼êµ´ì´ íƒ€ê²Ÿì¸ì§€ í™•ì¸ (Multi-Embedding Matching)

        Args:
            face_img_bgr: ì–¼êµ´ ì´ë¯¸ì§€ (BGR)
            target_embeddings: íƒ€ê²Ÿì˜ ë©”ì¸ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
            multi_embeddings: íƒ€ê²Ÿì˜ Multi-Thumbnail ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
            frame_idx: í˜„ì¬ í”„ë ˆì„ ë²ˆí˜¸ (ë¡œê¹…ìš©)

        Returns:
            (is_target, max_sim): (íƒ€ê²Ÿ ì—¬ë¶€, ìµœëŒ€ ìœ ì‚¬ë„)
        """
        if (not target_embeddings and not multi_embeddings) or face_img_bgr is None or face_img_bgr.size == 0:
            return False, 0.0

        # ì„ë² ë”© ì¶”ì¶œ
        embedding = self._get_embedding(face_img_bgr)
        if embedding is None:
            return False, 0.0

        # íƒ€ê²Ÿë“¤ê³¼ ë¹„êµ
        max_sim = -1.0
        max_sim_type = "none"

        # 1. ë©”ì¸ ì„ë² ë”©ê³¼ ë¹„êµ (í•­ìƒ ìˆ˜í–‰)
        for idx, target_emb in enumerate(target_embeddings):
            sim = np.dot(embedding, target_emb)
            if sim > max_sim:
                max_sim = sim
                max_sim_type = f"main_{idx}"

        # 2. Multi-Thumbnail ì„ë² ë”©ê³¼ ë¹„êµ (ì˜µì…˜)
        if self.use_multi_embedding:
            for target_idx, multi_emb_list in enumerate(multi_embeddings):
                for emb_idx, emb in enumerate(multi_emb_list):
                    emb_array = np.array(emb)
                    # L2 ì •ê·œí™”
                    emb_array = emb_array / (np.linalg.norm(emb_array) + 1e-8)
                    sim = np.dot(embedding, emb_array)
                    if sim > max_sim:
                        max_sim = sim
                        max_sim_type = f"multi_{target_idx}_{emb_idx}"

        # ì„ê³„ê°’ ë¹„êµ
        is_target = max_sim > self.threshold

        # ìƒì„¸ ë¡œê·¸ (100 í”„ë ˆì„ë§ˆë‹¤ ë˜ëŠ” threshold ê·¼ì²˜)
        if frame_idx % 100 == 0 or (max_sim > self.threshold - 0.1 and max_sim < self.threshold + 0.1):
            logger.info(
                f"  Frame {frame_idx}: similarity={max_sim:.3f} (threshold={self.threshold:.3f}, "
                f"type={max_sim_type}) â†’ {'PRESERVE' if is_target else 'BLUR'}"
            )

        return is_target, max_sim

    def _apply_blur(
        self,
        frame: np.ndarray,
        x1: int, y1: int, x2: int, y2: int,
        blur_type: str = 'pixelate',
        blur_strength: int = 15,
        padding: int = 20
    ) -> np.ndarray:
        """
        ì–¼êµ´ ì˜ì—­ì— ë¸”ëŸ¬ ì ìš©

        Args:
            frame: ì›ë³¸ í”„ë ˆì„
            x1, y1, x2, y2: ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤
            blur_type: 'pixelate' ë˜ëŠ” 'gaussian'
            blur_strength: ë¸”ëŸ¬ ê°•ë„
            padding: ì–¼êµ´ ì£¼ë³€ íŒ¨ë”©

        Returns:
            ë¸”ëŸ¬ ì²˜ë¦¬ëœ í”„ë ˆì„
        """
        h, w = frame.shape[:2]

        # íŒ¨ë”© ì¶”ê°€
        x1_pad = max(0, x1 - padding)
        y1_pad = max(0, y1 - padding)
        x2_pad = min(w, x2 + padding)
        y2_pad = min(h, y2 + padding)

        roi = frame[y1_pad:y2_pad, x1_pad:x2_pad]

        if roi.size == 0:
            return frame

        if blur_type == 'pixelate':
            # í”½ì…€í™” (ëª¨ìì´í¬)
            roi_h, roi_w = roi.shape[:2]
            factor = max(1, max(roi_h, roi_w) // blur_strength)

            if factor > 1:
                small = cv2.resize(roi, (roi_w // factor, roi_h // factor), interpolation=cv2.INTER_NEAREST)
                blurred_roi = cv2.resize(small, (roi_w, roi_h), interpolation=cv2.INTER_NEAREST)
            else:
                # ë„ˆë¬´ ì‘ìœ¼ë©´ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ fallback
                k_w = max((x2_pad - x1_pad) // 3 | 1, 3)
                k_h = max((y2_pad - y1_pad) // 3 | 1, 3)
                blurred_roi = cv2.GaussianBlur(roi, (k_w, k_h), 30)

        elif blur_type == 'gaussian':
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            k_w = max((x2_pad - x1_pad) // 3 | 1, 3)
            k_h = max((y2_pad - y1_pad) // 3 | 1, 3)
            blurred_roi = cv2.GaussianBlur(roi, (k_w, k_h), 30)

        else:
            # ê¸°ë³¸ê°’: ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            k_w = max((x2_pad - x1_pad) // 3 | 1, 3)
            k_h = max((y2_pad - y1_pad) // 3 | 1, 3)
            blurred_roi = cv2.GaussianBlur(roi, (k_w, k_h), 30)

        frame[y1_pad:y2_pad, x1_pad:x2_pad] = blurred_roi
        return frame

    def process_video(
        self,
        video_path: str,
        output_path: str,
        face_models: List[Dict],
        progress_callback: Optional[callable] = None,
        blur_type: str = 'pixelate',
        blur_strength: int = 15,
        threshold: float = 0.6
    ) -> bool:
        """
        ë¹„ë””ì˜¤ ë¸”ëŸ¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Optimized with Saved Tracking Data)

        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            output_path: ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            face_models: Face ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ [{'id', 'instance_id', 'frame_data', 'is_blurred'}, ...]
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
            blur_type: 'pixelate' ë˜ëŠ” 'gaussian'
            blur_strength: ë¸”ëŸ¬ ê°•ë„ (ë†’ì„ìˆ˜ë¡ ì•½í•¨)
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.6)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info("=" * 80)
            logger.info("ğŸ¬ Starting Video Blur Processing (Single-Pass)")
            logger.info("=" * 80)

            # threshold ì—…ë°ì´íŠ¸
            self.threshold = threshold

            # 1. í”„ë ˆì„ë³„ bbox ë°ì´í„° êµ¬ì¡°í™”
            # frame_faces[frame_idx] = [(instance_id, bbox, is_blurred), ...]
            frame_faces = {}

            logger.info(f"ğŸ“‹ Total face_models: {len(face_models)}")
            logger.info("ğŸ”„ Building frame-level bbox index...")

            for fm in face_models:
                is_blurred_val = fm.get('is_blurred', True)
                instance_id = fm.get('instance_id')
                frame_data = fm.get('frame_data', {})

                logger.info(f"Face instance {instance_id}: is_blurred={is_blurred_val}, frames={len(frame_data)}")

                # frame_data êµ¬ì¡°: {frame_idx: [x1, y1, x2, y2, conf], ...}
                for frame_idx_str, bbox_with_conf in frame_data.items():
                    frame_idx = int(frame_idx_str)

                    if frame_idx not in frame_faces:
                        frame_faces[frame_idx] = []

                    # bboxëŠ” [x1, y1, x2, y2, conf] í˜•íƒœ
                    x1, y1, x2, y2 = map(int, bbox_with_conf[:4])

                    frame_faces[frame_idx].append({
                        'instance_id': instance_id,
                        'bbox': (x1, y1, x2, y2),
                        'is_blurred': is_blurred_val
                    })

            logger.info(f"âœ… Indexed {len(frame_faces)} frames with face data")

            # í†µê³„
            total_indexed_faces = sum(len(faces) for faces in frame_faces.values())
            preserved_instances = sum(1 for fm in face_models if not fm.get('is_blurred', True))
            logger.info(f"ğŸ“Š Total indexed faces: {total_indexed_faces}")
            logger.info(f"ğŸ¯ Instances to preserve (not blur): {preserved_instances}")

            # 2. ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS) or 30.0

            logger.info(f"ğŸ“¹ Video: {width}x{height}, {fps} fps, {total_frames} frames")

            # 3. VideoWriter ìƒì„±
            temp_output = output_path + ".temp.mp4"
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))

            if not out.isOpened():
                raise RuntimeError(f"Failed to open VideoWriter: {temp_output}")

            # 4. í”„ë ˆì„ë³„ ì²˜ë¦¬ (ìµœì í™”: ì €ì¥ëœ bbox ì‚¬ìš©, YOLO/AdaFace ì¬ì‹¤í–‰ ë¶ˆí•„ìš”)
            frame_idx = 0
            blur_count = 0
            preserved_count = 0

            logger.info("ğŸï¸  Processing frames with saved tracking data...")

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # ì €ì¥ëœ bbox ë°ì´í„° ì‚¬ìš© (YOLO ì¬ì‹¤í–‰ ì—†ìŒ!)
                if frame_idx in frame_faces:
                    for face_info in frame_faces[frame_idx]:
                        x1, y1, x2, y2 = face_info['bbox']
                        is_blurred = face_info['is_blurred']

                        # ì¢Œí‘œ ë³´ì •
                        x1, y1 = max(0, x1), max(0, y1)
                        x2, y2 = min(width, x2), min(height, y2)

                        # ë„ˆë¬´ ì‘ì€ ì–¼êµ´ì€ ìŠ¤í‚µ
                        if (x2 - x1) < 20 or (y2 - y1) < 20:
                            continue

                        # â­ instance_id ê¸°ë°˜ ë¸”ëŸ¬ ê²°ì • (AdaFace ì¬ì‹¤í–‰ ì—†ìŒ!)
                        if is_blurred:
                            # is_blurred=True â†’ ë¸”ëŸ¬ ì²˜ë¦¬
                            frame = self._apply_blur(
                                frame, x1, y1, x2, y2,
                                blur_type=blur_type,
                                blur_strength=blur_strength
                            )
                            blur_count += 1
                        else:
                            # is_blurred=False â†’ ë³´ì¡´ (ë¸”ëŸ¬ ì—†ìŒ)
                            preserved_count += 1

                # í”„ë ˆì„ ì €ì¥
                out.write(frame)

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                frame_idx += 1
                if progress_callback and frame_idx % 30 == 0:
                    pct = int((frame_idx / total_frames) * 90)
                    pct = min(pct, 90)
                    progress_callback(pct)

                    if frame_idx % 300 == 0:
                        logger.info(
                            f"ğŸ“Š Processed {frame_idx}/{total_frames} frames | "
                            f"Blurred: {blur_count} | Preserved: {preserved_count}"
                        )

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if frame_idx % 500 == 0:
                    gc.collect()

            cap.release()
            out.release()

            logger.info(f"âœ… Processing completed: {frame_idx} frames")
            logger.info(f"ğŸ“Š Blurred: {blur_count}, Preserved: {preserved_count}")

            # 5. H.264 ì¸ì½”ë”©
            logger.info("ğŸï¸  Encoding to H.264...")
            self._encode_h264(temp_output, output_path)

            if progress_callback:
                progress_callback(100)

            logger.info("=" * 80)
            logger.info("âœ… Video processing completed successfully!")
            logger.info("=" * 80)

            return True

        except Exception as e:
            logger.error(f"âŒ Video processing failed: {e}", exc_info=True)
            return False

    def _encode_h264(self, input_path: str, output_path: str):
        """FFmpegë¡œ H.264 ì¸ì½”ë”©"""
        try:
            cmd = [
                'ffmpeg', '-y',
                '-i', input_path,
                '-c:v', 'libx264',
                '-preset', 'fast',
                '-crf', '23',
                '-c:a', 'aac',
                '-movflags', '+faststart',
                output_path
            ]

            logger.info(f"ğŸ¬ Running FFmpeg: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(input_path):
                os.remove(input_path)

            logger.info("âœ… FFmpeg encoding completed")

        except subprocess.TimeoutExpired:
            logger.error("âŒ FFmpeg encoding timeout")
            # Fallback: ì„ì‹œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if os.path.exists(input_path) and not os.path.exists(output_path):
                shutil.move(input_path, output_path)
                logger.warning("âš ï¸  Using temp file as output (FFmpeg timeout)")

        except Exception as e:
            logger.error(f"âŒ FFmpeg encoding failed: {e}")
            # Fallback: ì„ì‹œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if os.path.exists(input_path) and not os.path.exists(output_path):
                shutil.move(input_path, output_path)
                logger.warning("âš ï¸  Using temp file as output (FFmpeg failed)")
