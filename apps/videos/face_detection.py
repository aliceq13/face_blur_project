# -*- coding: utf-8 -*-
"""
ì–¼êµ´ ê°ì§€ ë° ì¶”ì  íŒŒì´í”„ë¼ì¸ (YOLO Face v11s + BoTSORT + AdaFace ViT-12M)

ì´ ëª¨ë“ˆì€ ìµœì‹  ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì—ì„œ ì–¼êµ´ì„ ê°ì§€í•˜ê³  ë™ì¼ ì¸ë¬¼ì„ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ìˆ :
1. YOLO Face v11s: ìµœì‹  ì–¼êµ´ ê°ì§€ ëª¨ë¸ (ë¹ ë¥´ê³  ì •í™•)
2. BoTSORT: ê³ ê¸‰ ì¶”ì  ì•Œê³ ë¦¬ì¦˜ (ByteTrack ê°œì„  ë²„ì „)
3. AdaFace ViT-12M: ìµœê³  ì„±ëŠ¥ì˜ ì–¼êµ´ ì¸ì‹ ëª¨ë¸
4. TW-FINCH/HDBSCAN: ì‹œê°„ ê°€ì¤‘ í´ëŸ¬ìŠ¤í„°ë§
5. Quality-Weighted Embedding: ì„ ëª…ë„ ê¸°ë°˜ ì„ë² ë”© í‰ê· 
"""

import os
import cv2
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from sklearn.preprocessing import normalize
import logging
import gc

logger = logging.getLogger(__name__)


class DetectedFace:
    """ê°ì§€ëœ ì–¼êµ´ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""

    def __init__(
        self,
        frame_idx: int,
        bbox: Tuple[int, int, int, int],
        confidence: float,
        face_img: np.ndarray,
        track_id: Optional[int] = None,
        embedding: Optional[np.ndarray] = None,
        clarity: float = 0.0
    ):
        self.frame_idx = frame_idx
        self.bbox = bbox
        self.confidence = confidence
        self.face_img = face_img
        self.track_id = track_id
        self.embedding = embedding
        self.clarity = clarity

    @property
    def bbox_area(self) -> int:
        """ë°”ìš´ë”© ë°•ìŠ¤ ë©´ì """
        x1, y1, x2, y2 = self.bbox
        return (x2 - x1) * (y2 - y1)


class Tracklet:
    """
    ë™ì¼í•œ Track IDë¥¼ ê°€ì§„ ì–¼êµ´ë“¤ì˜ ì§‘í•©
    ë©”ëª¨ë¦¬ ìµœì í™”: ìƒìœ„ 3ê°œ ì–¼êµ´ë§Œ ì´ë¯¸ì§€ë¡œ ì €ì¥, ë‚˜ë¨¸ì§€ëŠ” ì„ë² ë”©ë§Œ ì €ì¥
    """

    def __init__(self, track_id: int):
        self.track_id = track_id
        self.embeddings: List[np.ndarray] = []
        self.clarity_scores: List[float] = []
        self.top_faces: List[DetectedFace] = []  # ìƒìœ„ 3ê°œë§Œ ìœ ì§€
        self.avg_embedding: Optional[np.ndarray] = None
        self.appearance_count: int = 0
        self.first_frame: Optional[int] = None
        self.last_frame: Optional[int] = None

    def add_face(self, face: DetectedFace):
        """ì–¼êµ´ ì¶”ê°€ - ìƒìœ„ 3ê°œ ì„ ëª…í•œ ì–¼êµ´ ìœ ì§€"""
        self.appearance_count += 1

        if self.first_frame is None:
            self.first_frame = face.frame_idx
        self.last_frame = face.frame_idx

        # ì„ë² ë”© ì €ì¥
        if face.embedding is not None:
            self.embeddings.append(face.embedding)
            self.clarity_scores.append(face.clarity)

        # Top 3 ì–¼êµ´ ìœ ì§€
        self.top_faces.append(face)
        self.top_faces.sort(key=lambda x: x.clarity, reverse=True)

        if len(self.top_faces) > 3:
            # 4ìœ„ ì´í•˜ëŠ” ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ í•´ì œ
            for rm_face in self.top_faces[3:]:
                rm_face.face_img = None
            self.top_faces = self.top_faces[:3]

    def compute_average_embedding(self, outlier_threshold=0.7):
        """
        Quality-Weighted Averaging + Outlier Filtering

        Args:
            outlier_threshold: Cosine similarity threshold (ê¸°ë³¸ê°’ 0.7)
        """
        if not self.embeddings:
            return

        embeddings = np.array(self.embeddings)
        clarity_scores = np.array(self.clarity_scores)

        # Step 1: ë‹¨ìˆœ í‰ê·  ê³„ì‚° (outlier ê°ì§€ìš©)
        mean_emb = np.mean(embeddings, axis=0)
        mean_emb_norm = mean_emb / (np.linalg.norm(mean_emb) + 1e-8)

        # Step 2: Outlier Filtering
        similarities = np.dot(embeddings, mean_emb_norm)
        valid_mask = similarities >= outlier_threshold

        if not np.any(valid_mask):
            valid_embeddings = embeddings
            valid_clarity = clarity_scores
        else:
            valid_embeddings = embeddings[valid_mask]
            valid_clarity = clarity_scores[valid_mask]

        # Step 3: Quality-Weighted Averaging
        normalized_clarity = valid_clarity - np.max(valid_clarity)
        exp_clarity = np.exp(normalized_clarity)
        weights = exp_clarity / (np.sum(exp_clarity) + 1e-8)

        if np.any(np.isnan(weights)) or np.any(np.isinf(weights)):
            weights = np.ones(len(valid_embeddings)) / len(valid_embeddings)

        weighted_emb = np.sum(valid_embeddings * weights[:, np.newaxis], axis=0)

        # Step 4: L2 ì •ê·œí™”
        norm = np.linalg.norm(weighted_emb)
        if norm > 1e-8:
            self.avg_embedding = weighted_emb / norm
        else:
            self.avg_embedding = valid_embeddings[0] / (np.linalg.norm(valid_embeddings[0]) + 1e-8)

        # ë©”ëª¨ë¦¬ ì ˆì•½: ê°œë³„ ì„ë² ë”© ì‚­ì œ
        self.embeddings = []
        self.clarity_scores = []


class FaceDetectionPipeline:
    """
    ì–¼êµ´ ê°ì§€ ë° ì¸ì‹ íŒŒì´í”„ë¼ì¸

    YOLO Face v11s + BoTSORT + AdaFace ViT-12M ì‚¬ìš©
    """

    # ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬ ì„¤ì •
    CHUNK_SIZE = 2000  # 2000 í”„ë ˆì„ì”© ì²˜ë¦¬
    MEMORY_CHECK_INTERVAL = 200  # 200 í”„ë ˆì„ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì²´í¬
    MEMORY_LIMIT_GB = 7.0  # 7GB ì œí•œ

    def __init__(
        self,
        yolo_model_path: str = None,
        device: str = 'auto',
        sample_rate: int = 1  # ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬ ê¶Œì¥ (Tracking ì •í™•ë„)
    ):
        self.sample_rate = sample_rate
        self.frames_processed = 0

        # Device ì„¤ì •
        if device == 'auto':
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        logger.info(f"ğŸš€ Initializing Face Detection Pipeline on {self.device}")

        # YOLO ëª¨ë¸ ê²½ë¡œ
        if yolo_model_path is None:
            from django.conf import settings
            yolo_model_path = str(settings.YOLO_FACE_MODEL_PATH)

        # 1. YOLO Face v11s ë¡œë“œ
        from ultralytics import YOLO
        self.yolo_model = YOLO(yolo_model_path)
        self.yolo_model.to(self.device)
        logger.info(f"âœ… YOLO Face model loaded: {yolo_model_path}")

        # 2. AdaFace ViT-12M ë¡œë“œ
        try:
            from .adaface_wrapper import AdaFaceWrapper
            from django.conf import settings

            model_path = str(getattr(settings, 'ADAFACE_MODEL_PATH', None))
            model_arch = getattr(settings, 'ADAFACE_ARCHITECTURE', 'ir_101')

            if not model_path or not os.path.exists(model_path):
                raise FileNotFoundError(f"AdaFace model not found: {model_path}")

            self.face_recognizer = AdaFaceWrapper(
                model_path=model_path,
                architecture=model_arch,
                device=self.device
            )
            logger.info(f"âœ… AdaFace {model_arch} loaded from {model_path}")

        except Exception as e:
            logger.error(f"âŒ Failed to load AdaFace: {e}")
            self.face_recognizer = None

    def _calculate_clarity(self, img: np.ndarray) -> float:
        """ì´ë¯¸ì§€ ì„ ëª…ë„ ê³„ì‚° (Laplacian Variance)"""
        try:
            if img.size == 0:
                return 0.0
            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if len(img.shape) == 3 else img
            return float(cv2.Laplacian(gray, cv2.CV_64F).var())
        except Exception:
            return 0.0

    def _check_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ ë° ìë™ ì •ë¦¬"""
        import psutil

        process = psutil.Process()
        memory_gb = process.memory_info().rss / (1024 ** 3)

        if memory_gb > self.MEMORY_LIMIT_GB:
            logger.warning(f"âš ï¸  Memory high: {memory_gb:.2f} GB, cleaning...")
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        return memory_gb

    def _process_chunk(
        self,
        video_path: str,
        start_frame: int,
        end_frame: int,
        tracklets: Dict[int, Tracklet],
        conf_threshold: float = 0.5
    ) -> Tuple[int, int]:
        """
        ì²­í¬ ë‹¨ìœ„ë¡œ í”„ë ˆì„ ì²˜ë¦¬

        Returns:
            (embedding_success, embedding_fail) íŠœí”Œ
        """
        embedding_success = 0
        embedding_fail = 0

        # YOLO tracking ì‹¤í–‰ (BoTSORT)
        results = self.yolo_model.track(
            source=video_path,
            conf=0.4,  # ë‚®ì€ thresholdë¡œ ë” ë§ì€ ì–¼êµ´ ê°ì§€
            iou=0.5,
            persist=True,
            verbose=False,
            stream=True,
            vid_stride=self.sample_rate,
            device=self.device,
            tracker="botsort.yaml",  # BoTSORT ì‚¬ìš©
            imgsz=640,
            half=True  # FP16 ì‚¬ìš© (GPU ì„±ëŠ¥ í–¥ìƒ)
        )

        # ì²­í¬ ë²”ìœ„ ë‚´ í”„ë ˆì„ë§Œ ì²˜ë¦¬
        for idx, result in enumerate(results):
            frame_idx = start_frame + (idx * self.sample_rate)
            if frame_idx >= end_frame:
                break

            frame = result.orig_img

            if not result.boxes or result.boxes.id is None:
                continue

            for box in result.boxes:
                if box.id is None:
                    continue

                track_id = int(box.id.item())
                conf = float(box.conf.item())
                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())

                # ì¢Œí‘œ ë³´ì •
                h, w = frame.shape[:2]
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)

                if x2 <= x1 or y2 <= y1 or (x2 - x1) < 20 or (y2 - y1) < 20:
                    continue

                # ì–¼êµ´ í¬ë¡­
                face_img_bgr = frame[y1:y2, x1:x2]
                face_img_rgb = cv2.cvtColor(face_img_bgr, cv2.COLOR_BGR2RGB)
                clarity = self._calculate_clarity(face_img_rgb)

                detected_face = DetectedFace(
                    frame_idx=frame_idx,
                    bbox=(x1, y1, x2, y2),
                    confidence=conf,
                    face_img=face_img_rgb,
                    track_id=track_id,
                    clarity=clarity
                )

                # AdaFace ì„ë² ë”© ì¶”ì¶œ
                if self.face_recognizer:
                    embedding = self.face_recognizer.get_embedding(face_img_bgr)
                    if embedding is not None:
                        detected_face.embedding = embedding
                        embedding_success += 1
                    else:
                        embedding_fail += 1

                # Trackletì— ì¶”ê°€
                if track_id not in tracklets:
                    tracklets[track_id] = Tracklet(track_id)
                tracklets[track_id].add_face(detected_face)

            # ë©”ëª¨ë¦¬ ì²´í¬
            self.frames_processed += 1
            if self.frames_processed % self.MEMORY_CHECK_INTERVAL == 0:
                self._check_memory_usage()

        return embedding_success, embedding_fail

    def _cluster_tracklets(
        self,
        valid_tracklets: List[Tracklet],
        embeddings: List[np.ndarray],
        method: str = 'finch',
        sim_threshold: float = 0.6
    ) -> np.ndarray:
        """
        Tracklet í´ëŸ¬ìŠ¤í„°ë§

        Args:
            valid_tracklets: ìœ íš¨í•œ tracklet ë¦¬ìŠ¤íŠ¸
            embeddings: ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
            method: 'finch', 'tw-finch', 'hdbscan', 'hac'
            sim_threshold: HAC ìœ ì‚¬ë„ ì„ê³„ê°’

        Returns:
            labels: í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸” ë°°ì—´
        """
        if len(valid_tracklets) == 1:
            logger.info("Only 1 tracklet, skipping clustering")
            return np.array([0])

        embeddings_array = np.array(embeddings)

        # FINCH: Parameter-free clustering
        if method == 'finch':
            try:
                from finch import FINCH
                logger.info("ğŸ” Using FINCH clustering (parameter-free)")

                c, num_clust, req_c = FINCH(embeddings_array, distance='cosine', verbose=False)
                req_c = req_c if req_c is not None else len(num_clust) - 1
                labels = c[:, req_c]

                logger.info(f"âœ… FINCH: {num_clust[req_c]} unique persons (level {req_c})")
                return labels

            except ImportError:
                logger.warning("FINCH not available, falling back to HAC")
                method = 'hac'

        # TW-FINCH: Time-Weighted FINCH
        if method == 'tw-finch':
            try:
                from finch import FINCH
                from scipy.spatial.distance import pdist, squareform

                logger.info("ğŸ” Using TW-FINCH (time-weighted)")

                # ì‹œê°„ feature ì¶”ê°€
                temporal_features = []
                fps = 30
                for t in valid_tracklets:
                    mid_time = (t.first_frame + t.last_frame) / 2 / fps
                    temporal_features.append([mid_time])

                temporal_features = np.array(temporal_features)
                temporal_range = temporal_features.max() - temporal_features.min()

                if temporal_range > 1e-6:
                    temporal_features = (temporal_features - temporal_features.min()) / temporal_range
                else:
                    temporal_features = np.zeros_like(temporal_features)

                # Embedding + temporal feature
                enhanced_embeddings = np.concatenate([
                    embeddings_array,
                    0.1 * temporal_features
                ], axis=1)

                c, num_clust, req_c = FINCH(enhanced_embeddings, distance='cosine', verbose=False)
                req_c = req_c if req_c is not None else len(num_clust) - 1
                labels = c[:, req_c]

                logger.info(f"âœ… TW-FINCH: {num_clust[req_c]} unique persons (level {req_c})")
                return labels

            except ImportError:
                logger.warning("FINCH not available, falling back to HAC")
                method = 'hac'

        # HDBSCAN: State-of-the-art clustering
        if method == 'hdbscan':
            try:
                import hdbscan
                logger.info("ğŸ” Using HDBSCAN clustering")

                embeddings_array = embeddings_array.astype('float64')
                embeddings_array = normalize(embeddings_array, norm='l2', axis=1).astype('float64')

                clusterer = hdbscan.HDBSCAN(
                    min_cluster_size=3,
                    min_samples=2,
                    metric='euclidean',
                    cluster_selection_method='eom'
                )

                labels = clusterer.fit_predict(embeddings_array)

                # ë…¸ì´ì¦ˆ(-1) ì²˜ë¦¬
                if -1 in labels:
                    noise_indices = np.where(labels == -1)[0]
                    max_label = labels.max()
                    for idx, noise_idx in enumerate(noise_indices):
                        labels[noise_idx] = max_label + 1 + idx

                num_clusters = len(set(labels))
                logger.info(f"âœ… HDBSCAN: {num_clusters} unique persons")
                return labels

            except ImportError:
                logger.warning("HDBSCAN not available, falling back to HAC")
                method = 'hac'

        # HAC: Fallback method
        logger.info("ğŸ” Using HAC clustering")
        from sklearn.cluster import AgglomerativeClustering

        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=1.0 - sim_threshold,
            metric='cosine',
            linkage='average'
        )
        labels = clustering.fit_predict(embeddings_array)

        num_clusters = len(set(labels))
        logger.info(f"âœ… HAC: {num_clusters} unique persons")
        return labels

    def process_video(
        self,
        video_path: str,
        output_dir: str,
        conf_threshold: float = 0.5,
        sim_threshold: float = 0.6,
        clustering_method: str = 'finch',
        progress_callback: Optional[callable] = None
    ) -> List[Dict]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Tracking -> Embedding -> Clustering -> Thumbnail Generation
        """
        logger.info("=" * 80)
        logger.info(f"ğŸ¬ Starting Face Detection Pipeline: {video_path}")
        logger.info("=" * 80)

        # ë¹„ë””ì˜¤ ì •ë³´
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()

        logger.info(f"ğŸ“¹ Total frames: {total_frames}, chunk size: {self.CHUNK_SIZE}")

        # ì²­í¬ ê³„ì‚°
        num_chunks = (total_frames + self.CHUNK_SIZE - 1) // self.CHUNK_SIZE
        tracklets: Dict[int, Tracklet] = {}

        total_embedding_success = 0
        total_embedding_fail = 0

        # ì²­í¬ë³„ ì²˜ë¦¬
        for chunk_idx in range(num_chunks):
            start_frame = chunk_idx * self.CHUNK_SIZE
            end_frame = min((chunk_idx + 1) * self.CHUNK_SIZE, total_frames)

            logger.info(f"ğŸ“¦ Chunk {chunk_idx + 1}/{num_chunks} (frames {start_frame}-{end_frame})")

            success, fail = self._process_chunk(
                video_path, start_frame, end_frame, tracklets, conf_threshold
            )

            total_embedding_success += success
            total_embedding_fail += fail

            if progress_callback:
                progress_pct = int((end_frame / total_frames) * 80)  # 80%ê¹Œì§€
                progress_callback(progress_pct, f"Chunk {chunk_idx + 1}/{num_chunks} ì™„ë£Œ")

            # ì²­í¬ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        logger.info(f"âœ… Tracking completed: {len(tracklets)} tracklets")
        logger.info(f"ğŸ“Š Embedding: {total_embedding_success} success, {total_embedding_fail} fail")

        # Tracklet í‰ê·  ì„ë² ë”© ê³„ì‚°
        valid_tracklets = []
        embeddings = []

        for t in tracklets.values():
            t.compute_average_embedding()
            if t.avg_embedding is not None:
                valid_tracklets.append(t)
                embeddings.append(t.avg_embedding)

        if not valid_tracklets:
            logger.warning("âš ï¸  No valid tracklets found")
            return []

        logger.info(f"ğŸ¯ Valid tracklets: {len(valid_tracklets)}")

        # í´ëŸ¬ìŠ¤í„°ë§
        labels = self._cluster_tracklets(
            valid_tracklets, embeddings, clustering_method, sim_threshold
        )

        # í´ëŸ¬ìŠ¤í„°ë³„ ê·¸ë£¹í™”
        clusters: Dict[int, List[Tracklet]] = {}
        for tracklet, label in zip(valid_tracklets, labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(tracklet)

        logger.info(f"ğŸ‘¥ Unique persons: {len(clusters)}")

        if progress_callback:
            progress_callback(90, "ì¸ë„¤ì¼ ìƒì„± ì¤‘...")

        # ì¸ë„¤ì¼ ìƒì„±
        result_faces = []
        face_index = 1

        for cluster_id, cluster_tracklets in clusters.items():
            # í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  Top ì–¼êµ´ ìˆ˜ì§‘
            all_top_faces = []
            for t in cluster_tracklets:
                all_top_faces.extend(t.top_faces)

            if not all_top_faces:
                continue

            # ì„ ëª…ë„ ìˆœ ì •ë ¬
            all_top_faces.sort(key=lambda x: x.clarity, reverse=True)
            final_top_3 = all_top_faces[:3]
            best_face = final_top_3[0]

            # ì¸ë„¤ì¼ ì €ì¥
            thumbnail_filename = f"face_{face_index}.jpg"
            thumbnail_path = os.path.join(output_dir, thumbnail_filename)
            self._save_thumbnail(best_face, thumbnail_path)

            # í”„ë ˆì„ ì •ë³´
            all_first_frames = [t.first_frame for t in cluster_tracklets if t.first_frame is not None]
            all_last_frames = [t.last_frame for t in cluster_tracklets if t.last_frame is not None]
            total_appearances = sum(t.appearance_count for t in cluster_tracklets)

            # í´ëŸ¬ìŠ¤í„° í‰ê·  ì„ë² ë”©
            cluster_avg_emb = np.mean([t.avg_embedding for t in cluster_tracklets], axis=0)
            cluster_avg_emb = normalize(cluster_avg_emb.reshape(1, -1))[0]

            # Multi-thumbnail ì„ë² ë”© (ìƒìœ„ 3ê°œ)
            multi_embeddings = []
            for face in final_top_3:
                if face.embedding is not None:
                    multi_embeddings.append(face.embedding.tolist())

            result_faces.append({
                'face_index': face_index,
                'thumbnail_path': thumbnail_path,
                'embedding': cluster_avg_emb.tolist(),
                'embeddings': multi_embeddings,  # Multi-thumbnail
                'appearance_count': total_appearances,
                'first_frame': min(all_first_frames) if all_first_frames else 0,
                'last_frame': max(all_last_frames) if all_last_frames else 0
            })

            face_index += 1

        if progress_callback:
            progress_callback(100, "ì™„ë£Œ")

        logger.info("=" * 80)
        logger.info(f"âœ… Pipeline completed: {len(result_faces)} unique faces")
        logger.info("=" * 80)

        return result_faces

    def _save_thumbnail(self, face: DetectedFace, output_path: str, size=(160, 160)):
        """ì¸ë„¤ì¼ ì €ì¥"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        resized = cv2.resize(face.face_img, size, interpolation=cv2.INTER_AREA)
        bgr_img = cv2.cvtColor(resized, cv2.COLOR_RGB2BGR)
        cv2.imwrite(output_path, bgr_img)
